{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order2 = (-rnd_clf_holder_nbl[0][-1][4].feature_importances_).argsort(axis=0)\n",
    "(order2 == order).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "order = (-importances_full[:,0,-1]).argsort(axis=0)\n",
    "#tmp = \n",
    "order = (-tmp[:,-1]).argsort(axis=0)\n",
    "#scores = importances_full.mean(axis=(1,2))\n",
    "scores = importances_full.mean(axis=(1,2))\n",
    "order = (-scores).argsort(axis=0)\n",
    "(np.abs(scores[order]-important_genes_nbl)<1e-10).all()\n",
    "#[x.split('.')[0] for x in X2.columns[order]][:10]# == important_genes_nbl.index\n",
    "([x.split('.')[0] for x in X2.columns[order]][:] == important_genes_nbl.index[:]).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(importances_full.mean(axis=(1,2)) - importances_full.mean(axis=1).mean(axis=1))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_genes_nbl, importances_full = calculate_average_feature_importance(X2.columns, rnd_clf_holder_nbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_feature_importance(feature_names, rnd_clf_holder, num_last_sample_sizes=10):\n",
    "\n",
    "    # Sample call: calculate_average_feature_importance(X2.columns, rnd_clf_holder)\n",
    "\n",
    "    # Import relevant libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get the big numpy array of feature importances\n",
    "    importances = np.zeros((len(feature_names), len(rnd_clf_holder), len(rnd_clf_holder[0])))\n",
    "    for itrial, rnd_clf_holder_inside in enumerate(rnd_clf_holder): # ntrials of these\n",
    "        for isample_size, model_data in enumerate(rnd_clf_holder_inside): # len(possible_n) of these; model_data is [itrial, iin, n, y_bal, clf]\n",
    "            importances[:,itrial,isample_size] = model_data[4].feature_importances_\n",
    "\n",
    "    # Save the full importances holder\n",
    "    importances_full = importances.copy()\n",
    "\n",
    "    # Average over the trials and the last num_last_sample_sizes sample sizes\n",
    "    importances = importances[:,:,-num_last_sample_sizes:].mean(axis=(1,2))\n",
    "\n",
    "    # Sort the importances in decreasing order\n",
    "    order = (-importances).argsort(axis=0)\n",
    "\n",
    "    # Create a Pandas series of the average importances with the corresponding gene names as indexes in descending order, removing the version numbers from the Ensembl gene IDs\n",
    "    important_genes = pd.Series(data=importances[order], index=[x.split('.')[0] for x in feature_names[order]], name='score')\n",
    "\n",
    "    return(important_genes, importances_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_genes_nbl['ENSG00000164853']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_genes = important_genes_all\n",
    "important_genes_aml[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_genes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "num_last_sample_sizes = 10\n",
    "importances = np.zeros(tuple([X2.shape[1]] + list(accuracies.shape)))\n",
    "for itrial, rnd_clf_holder_inside in enumerate(rnd_clf_holder): # ntrials of these\n",
    "    for isample_size, model_data in enumerate(rnd_clf_holder_inside): # len(possible_n) of these; model_data is [itrial, iin, n, y_bal, clf]\n",
    "        importances[:,itrial,isample_size] = model_data[4].feature_importances_\n",
    "importances = importances[:,:,-num_last_sample_sizes:].mean(axis=(1,2))\n",
    "order = (-importances).argsort(axis=0) # sort the importances in decreasing order\n",
    "important_genes = pd.Series(data=importances[order], index=[x.split('.')[0] for x in X2.columns[order]], name='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_sample_size(accuracies, possible_n, study_name):\n",
    "\n",
    "    # Import relevant libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Calculate the average accuracy over all the trials\n",
    "    means = accuracies.mean(axis=0)\n",
    "\n",
    "    # Plot the mean accuracy vs. the sampling sizes with error bars indicating the minimum and maximum accuracies over all the trials\n",
    "    _, ax = plt.subplots(figsize=(10,6))\n",
    "    _ = ax.errorbar(x=possible_n, y=means, yerr=np.row_stack((means-accuracies.min(axis=0), accuracies.max(axis=0)-means)), fmt='*-', ecolor='red', capsize=4)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('Sample size for each class')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Overall accuracy on input dataset - ' + study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.iloc[nbl_or_normal_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbl_or_normal_samples = (y2=='NBL').to_numpy() | (y2=='Normal').to_numpy()\n",
    "aml_or_normal_samples = (y2=='AML').to_numpy() | (y2=='Normal').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.split('.')[0] for x in X2.columns[order][:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (-importances[:,:,-10:].mean(axis=(1,2))).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(averages[order[:20],-10:nn])\n",
    "tmp = averages[:,-10:nn].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (-averages[:,nn-1]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = importances.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (-tmp).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = importances[:,0,nn-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ntrials = len(rnd_clf_holder)\n",
    "nn = len(possible_n)\n",
    "importances = np.zeros((X2.shape[1], ntrials, nn))\n",
    "for itrial, rnd_clf_holder_inside in enumerate(rnd_clf_holder):\n",
    "    for iin, rnd_clf in enumerate(rnd_clf_holder_inside):\n",
    "        importances[:,itrial,iin] = rnd_clf[4].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tci = tc.get_tci_library()\n",
    "tci.make_pickle([X2, y2], '/home/weismanal/notebook/2020-08-09/writing_rf_models', 'data_for_writing_rf_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = rnd_clf_holder[0][0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.value_counts(rnd_clf.feature_importances_)\n",
    "import pandas as pd\n",
    "pd.value_counts(rnd_clf.feature_importances_).sort_index()\n",
    "#rnd_clf.feature_importances_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "means = accuracies.mean(axis=0)\n",
    "_, ax = plt.subplots(figsize=(10,6))\n",
    "_ = ax.errorbar(x=possible_n, y=means, yerr=np.row_stack((means-accuracies.min(axis=0), accuracies.max(axis=0)-means)), fmt='*-', ecolor='red', capsize=4)\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Sample size for each class')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Overall accuracy on entire dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_hist = df_samples.hist(figsize=(12,8))\n",
    "\n",
    "# \"Constants\" based on viewing the first set of histograms above so that we can figure out which ones to use to filter the data\n",
    "columns = ['average base quality', 'proportion_base_mismatch', 'proportion_reads_mapped']\n",
    "higher_is_better = [True, False, True]\n",
    "sp_locs = [(0,0), (1,1), (2,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_hist[0,1].title.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ax in ax_hist.flatten():\n",
    "    if ax.title.get_text() == 'proportion_base_mismatch':\n",
    "        break\n",
    "print(ax.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the coverage shown above, it seems like trying `n=200` is a reasonable starting place; it could be better, it could be worse.\n",
    "\n",
    "## Obtain an appropriately sized, balanced dataset\n",
    "\n",
    "n = 200\n",
    "X, y, num_indexes = tc.sample_populations(X2, y2, n=n)\n",
    "\n",
    "ax = tc.plot_unsupervised_analysis(tsne_res[num_indexes,:], y2.iloc[num_indexes], alpha=0.5)\n",
    "ax.set_title('tSNE - VST - n={} - sample that we\\'re using'.format(n))\n",
    "\n",
    "ax = tc.plot_unsupervised_analysis(tsne_res, y2, gray_indexes=num_indexes)\n",
    "ax.set_title('tSNE - VST - n={} - sample that we\\'re using in gray on top of original'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.columns.equals(X2.columns)\n",
    "#X.index.equals(y.index)\n",
    "#y_small = y[::20]\n",
    "#X_small = X.iloc[::20,:500]\n",
    "#X_small.index.equals(y_small.index)\n",
    "#X_small.columns.equals(X2.columns[:500])\n",
    "#y_small.value_counts()\n",
    "#X_small\n",
    "# import sklearn.ensemble as sk_ens\n",
    "clf = sk_ens.RandomForestClassifier(verbose=1)\n",
    "#step = 1\n",
    "#ngenes = 65000\n",
    "#clf.fit(X.iloc[::step,:ngenes], y[::step])\n",
    "clf.fit(X, y)\n",
    "#clf.predict_proba(X_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _ = tc.sample_populations(X2, y2, n=50)\n",
    "clf = sk_ens.RandomForestClassifier(verbose=1)\n",
    "clf.fit(X,y)\n",
    "clf.score(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.ensemble as sk_ens\n",
    "possible_n = [x for x in range(1,13)] + [x for x in range(15,46,5)] + [x for x in range(50,101,10)]\n",
    "ntrials = 10\n",
    "accuracies = np.zeros((ntrials, len(possible_n)))\n",
    "for itrial in range(ntrials):\n",
    "    for iin, n in enumerate(possible_n):\n",
    "        X, y, _ = tc.sample_populations(X2, y2, n=n)\n",
    "        clf = sk_ens.RandomForestClassifier()\n",
    "        clf.fit(X,y)\n",
    "        accuracies[itiral, iin] = clf.score(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(os.path.join(project_directory, 'data', 'exploring_sample_size_100_601_100.png'), facecolor='w', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "base_figsize = (16,5) # this is the size of an entire two-image row\n",
    "n_range = range(100,601,200)\n",
    "n_values = [x for x in n_range]\n",
    "nn = len(n_values)\n",
    "_, axs = plt.subplots(nrows=nn, ncols=2, figsize=(base_figsize[0], base_figsize[1]*nn), squeeze=False)\n",
    "for n_ind, n in enumerate(n_values):\n",
    "    # Sample the imbalanced dataset, returning the balanced dataset and reproducing indexes\n",
    "    _, _, num_indexes = tc.sample_populations(X2, y2, n=n)\n",
    "\n",
    "    # Plot the sampling results themselves\n",
    "    ax = tc.plot_unsupervised_analysis(tsne_res[num_indexes,:], y2.iloc[num_indexes], alpha=0.5, ax=axs[n_ind,0]) # note y = y2.iloc[num_indexes]\n",
    "    ax.set_title('tSNE - VST - n={} - sample'.format(n))\n",
    "\n",
    "    # Now plot the sampling results on top of the tSNE of the full dataset in order to see how much we covered\n",
    "    ax = tc.plot_unsupervised_analysis(tsne_res, y2, gray_indexes=num_indexes, ax=axs[n_ind,1])\n",
    "    ax.set_title('tSNE - VST - n={} - sample in gray on top of original'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "# Also, add tc. to the two functions below (in three places total)\n",
    "\n",
    "# Sample the imbalanced dataset, returning the balanced dataset and reproducing indexes\n",
    "X, y, num_indexes = sample_populations(X2, y2, n=n)\n",
    "\n",
    "# Plot the sampling results themselves\n",
    "ax = plot_unsupervised_analysis(tsne_res[num_indexes,:], y2.iloc[num_indexes], alpha=0.5) # note y = y2.iloc[num_indexes]\n",
    "ax.set_title('tSNE - VST - sample')\n",
    "\n",
    "# Now plot the sampling results on top of the tSNE of the full dataset in order to see how much we covered\n",
    "ax = plot_unsupervised_analysis(tsne_res, y2, gray_indexes=num_indexes)\n",
    "ax.set_title('tSNE - VST - sample in gray on top of original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs = [0.5, 0.75, 1, 1.25, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a PCA or tSNE analysis\n",
    "def plot_unsupervised_analysis(results, y, figsize=(12,7.5), alpha=1, gray_indexes=None):\n",
    "\n",
    "    # Sample calls:\n",
    "    #\n",
    "    #   # Perform PCA\n",
    "    #   import sklearn.decomposition as sk_decomp\n",
    "    #   pca = sk_decomp.PCA(n_components=10)\n",
    "    #   pca_res = pca.fit_transform(X.iloc[:,:500])\n",
    "    #   print('Top {} PCA explained variance ratios: {}'.format(10, pca.explained_variance_ratio_))\n",
    "    #   ax = tc.plot_unsupervised_analysis(pca_res, y)\n",
    "    #   ax.set_title('PCA - variance-stabilizing transformation')\n",
    "    #\n",
    "    #   # Perform tSNE analysis\n",
    "    #   import sklearn.manifold as sk_manif\n",
    "    #   tsne = sk_manif.TSNE(n_components=2)\n",
    "    #   tsne_res = tsne.fit_transform(X.iloc[:,:500])\n",
    "    #   ax = tc.plot_unsupervised_analysis(tsne_res, y)\n",
    "    #   ax.set_title('tSNE - variance-stabilizing transformation')\n",
    "    #\n",
    "\n",
    "    # Import relevant libraries\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.lines as mpl_lines\n",
    "\n",
    "    # Get a reasonable set of markers and color palette\n",
    "    markers = mpl_lines.Line2D.filled_markers\n",
    "    nclasses = len(set(y))\n",
    "    marker_list = markers * int(nclasses/len(markers)+1)\n",
    "    color_palette = sns.color_palette(\"hls\", nclasses)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=figsize)\n",
    "    #ax = sns.scatterplot(x=results[:,0], y=results[:,1], hue=y, style=y, palette=color_palette, legend=\"full\", alpha=alpha, markers=marker_list, edgecolor='k')\n",
    "    ax = sns.scatterplot(x=results[:,0], y=results[:,1], hue=y, style=y, palette=color_palette, legend=\"full\", alpha=(0.2 if gray_indexes is not None else alpha), markers=marker_list, edgecolor='k')\n",
    "\n",
    "    if gray_indexes is not None:\n",
    "        import collections\n",
    "        gray_indexes=list(collections.OrderedDict.fromkeys(gray_indexes.to_list()))\n",
    "        #ax = sns.scatterplot(x=results[gray_indexes,0], y=results[gray_indexes,1], hue='gray', style=y.iloc[gray_indexes], palette=color_palette, markers=marker_list, edgecolor='k', ax=ax)\n",
    "        ax = sns.scatterplot(x=results[gray_indexes,0], y=results[gray_indexes,1], color='gray', style=y.iloc[gray_indexes], palette=color_palette, markers=marker_list, edgecolor='k', ax=ax, alpha=1, legend=\"full\")\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "    # if save_figure:\n",
    "    #     fig.savefig(os.path.join(data_dir, 'pca_or_tsne_' + transformation_name_filename + '_transformation' + fn_addendum + '.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "    return(ax)\n",
    "\n",
    "\n",
    "# Sample with replacement each label-group of the potentially unbalanced inputted data matrix and corresponding labels\n",
    "# Return the corresponding balanced data matrix and corresponding labels, along with the numerical indexes that could be used to obtain these\n",
    "def sample_populations(X2, y2, n=10):\n",
    "\n",
    "    # Ensure the indexes of the input matrix and array match\n",
    "    if not y2.index.equals(X2.index):\n",
    "        print('ERROR: Indexes of input X and y do not match')\n",
    "        exit()\n",
    "\n",
    "    # Initialize the data matrix\n",
    "    X = X2.copy()\n",
    "\n",
    "    # Add the column of labels to the data matrix\n",
    "    X['label'] = y2.copy()\n",
    "\n",
    "    # Also add a column of the numerical indexes corresponding to the samples in X2 and y2\n",
    "    X['index2'] = range(len(X))\n",
    "\n",
    "    # Check that we did what we think we did\n",
    "    if (not (X.iloc[:,-2] == y2).all()) or (not (X.iloc[:,:-2] == X2).all().all()):\n",
    "        print('ERROR: We didn\\'t correctly place the data and label matrices inside the combined data matrix')\n",
    "        exit()\n",
    "\n",
    "    # Sample with replacement each group (unique label) within the combined data matrix\n",
    "    X = X.groupby('label').sample(n=n, replace=True)\n",
    "\n",
    "    # Save the sampled labels and numerical indexes and drop those columns from the combined data matrix\n",
    "    y = X['label']\n",
    "    num_indexes = X['index2']\n",
    "    X = X.drop(['label', 'index2'], axis='columns')\n",
    "\n",
    "    # Ensure that all we really need is num_indexes\n",
    "    if (not (y2.iloc[num_indexes] == y).all()) or (not (X2.iloc[num_indexes,:] == X).all().all()):\n",
    "        print('ERROR: We cannot reproduce the results from num_indexes alone, as we should be able')\n",
    "        exit()\n",
    "\n",
    "    # Return the balanced data and labels and reproducing numerical indexes\n",
    "    return(X, y, num_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Plot a PCA or tSNE analysis\n",
    "#def plot_unsupervised_analysis(results, y, figsize=(12,7.5), alpha=1, gray_indexes=None):\n",
    "\n",
    "results=tsne_res[num_indexes,:]\n",
    "#results=tsne_res\n",
    "y=y2.iloc[num_indexes]\n",
    "#y=y2\n",
    "figsize=(12,7.5)\n",
    "alpha=1\n",
    "gray_indexes = None\n",
    "#gray_indexes=num_indexes # default=None\n",
    "\n",
    "# Import relevant libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mpl_lines\n",
    "\n",
    "# Get a reasonable set of markers and color palette\n",
    "markers = mpl_lines.Line2D.filled_markers\n",
    "nclasses = len(set(y))\n",
    "marker_list = markers * int(nclasses/len(markers)+1)\n",
    "color_palette = sns.color_palette(\"hls\", nclasses)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=figsize)\n",
    "ax = sns.scatterplot(x=results[:,0], y=results[:,1], hue=y, style=y, palette=color_palette, legend=\"full\", alpha=(0.1 if gray_indexes is not None else alpha), markers=marker_list, edgecolor='k')\n",
    "\n",
    "if gray_indexes is not None:\n",
    "    import collections\n",
    "    gray_indexes=list(collections.OrderedDict.fromkeys(gray_indexes.to_list()))\n",
    "    ax = sns.scatterplot(x=results[gray_indexes,0], y=results[gray_indexes,1], color='gray', style=y.iloc[gray_indexes], palette=color_palette, markers=marker_list, edgecolor='k', ax=ax, alpha=0.6, legend=\"full\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 10\n",
    "#X = X2.copy()\n",
    "#y2.index.equals(X.index)\n",
    "#X['label'] = y2.copy()\n",
    "#X['index2'] = range(len(X))\n",
    "#tmp = X.groupby('label').sample(n=n, replace=True)\n",
    "#(X.iloc[:,:-2] == X2).all().all()\n",
    "#(X.iloc[:,-2] == y2).all()\n",
    "#tmp = tmp.rename({'index orig': 'index2'}, axis=1)\n",
    "#ax = tc.plot_unsupervised_analysis(tsne_res[tmp['index2'],:], y2.iloc[tmp['index2']], alpha=0.5)\n",
    "#ax.set_title('tSNE - variance-stabilizing transformation - sample')\n",
    "#(y2.iloc[tmp['index2']] == tmp['label']).all()\n",
    "#list(set(tmp['index2'].to_list()))\n",
    "# gray_indexes = list(set(tmp['index2'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_indexes = y.to_numpy().argsort(axis=0)\n",
    "y.iloc[sorting_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old cell\n",
    "\n",
    "## Write annotation and gene counts files (two files total) that are in the same format as the pasilla example\n",
    "\n",
    "# Commenting this out since the sample we're taking is random and I don't want to regenerate it as I didn't save the seed\n",
    "# Don't stress too much about that since this is super easy to regenerate and I should probably implement saving the seed\n",
    "#tc.write_sample_for_deseq2_input(df_samples['label 1'], df_counts, data_directory, 'three_class_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r_script_dir, dataset_name = checkout_dir, 'all_data_label_2'\n",
    "cmd_list = ['Rscript', '--vanilla', os.path.join(r_script_dir, 'run_vst.R'), dataset_name]\n",
    "print('Now running command: ' + ' '.join(cmd_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(['Rscript', '--vanilla', os.path.join('/home/andrew/weisman', 'run_vst.R'), 'lee_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples['label 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old block in Main:\n",
    "\n",
    "Now run `main_r.ipynb` with `dataset_name` set to `all_data`\n",
    "\n",
    "The files created that are important to the next block are, e.g.:\n",
    "\n",
    "`/data/BIDS-HPC/private/projects/dmi2/data/datasets/all_data/assay_variance_stabilizing_transformation.csv`  \n",
    "`/data/BIDS-HPC/private/projects/dmi2/data/datasets/all_data/coldata_variance_stabilizing_transformation.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_samples['label 2']\n",
    "y.index = y.index.str.replace('-', '_')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_samples['label 2'] = df_samples['label 1']\n",
    "#print(df_samples['label 2'].value_counts())\n",
    "#df_samples['label 2'][df_samples['label 2'].str.contains('normal', case=False, regex=False)] = 'TARGET-Normal'\n",
    "#print(df_samples['label 2'].value_counts())\n",
    "#df_samples['label 2'] = df_samples['label 2'].str.split(pat=', ', expand=True)[0]\n",
    "#print(df_samples['label 2'].value_counts())\n",
    "#df_samples['label 2'] = df_samples['label 2'].str.split(pat='-', expand=True)[1]\n",
    "print(df_samples['label 2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'normal transformation'.lower().replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/data/BIDS-HPC/private/projects/dmi2/data/datasets/all_data/assay_normal_transformation.csv')\n",
    "df = df.set_index('Unnamed: 0').sort_index()\n",
    "top_indexes = df.var(axis=1).sort_values(axis=0, ascending=False)[:1000].index\n",
    "X = df.loc[top_indexes,:].T\n",
    "df = pd.read_csv('/data/BIDS-HPC/private/projects/dmi2/data/datasets/all_data/coldata_normal_transformation.csv')\n",
    "df = df.set_index('Unnamed: 0').sort_index()\n",
    "y = df.loc[X.index,:]\n",
    "sample_order = y['condition'].sort_values().index\n",
    "y = y.loc[sample_order,:]\n",
    "X = X.loc[sample_order,:]\n",
    "print(y.index.equals(X.index))\n",
    "import sklearn.decomposition as sk_decomp\n",
    "pca = sk_decomp.PCA(n_components=10)\n",
    "pca_res = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,7.5))\n",
    "#sns.scatterplot(x=\"pca-one\", y=\"pca-two\", hue=\"y\", palette=sns.color_palette(\"hls\", len(set(y['condition']))), data=df, legend=\"full\", alpha=0.3)\n",
    "marker_list = ['.', ',', 'o', 'v', '^', '<', '>']*4\n",
    "from matplotlib.lines import Line2D\n",
    "marker_list2 = list(Line2D.markers.keys())[:37]\n",
    "markers = Line2D.filled_markers\n",
    "nclasses = len(set(y['condition']))\n",
    "marker_list3 = markers * int(nclasses/len(markers)+1)\n",
    "ax = sns.scatterplot(x=pca_res[:,0], y=-pca_res[:,1], hue=y['condition'], style=y['condition'], palette=sns.color_palette(\"hls\", nclasses), legend=\"full\", alpha=1, markers=marker_list3, edgecolor='k') # , size=y['condition']\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "ax.set_title('PCA - normal transformation')\n",
    "fig.savefig('/data/BIDS-HPC/private/projects/dmi2/data/datasets/all_data/pca_normal_transformation.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.manifold as sk_manif\n",
    "tsne = sk_manif.TSNE(n_components=2, verbose=1) #, perplexity=40, n_iter=300)\n",
    "tsne_res = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7.5))\n",
    "ax = sns.scatterplot(x=tsne_res[:,0], y=tsne_res[:,1], hue=y['condition'], style=y['condition'], palette=sns.color_palette(\"hls\", nclasses), legend=\"full\", alpha=1, markers=marker_list3, edgecolor='k') # , size=y['condition']\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "ax.set_title('tSNE - normal transformation')\n",
    "fig.savefig('/data/BIDS-HPC/private/projects/dmi2/data/datasets/all_data/tsne_normal_transformation.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca50 = sk_decomp.PCA(n_components=50)\n",
    "pca50_res = pca50.fit_transform(X)\n",
    "tsne50 = sk_manif.TSNE(n_components=2, verbose=1) #, perplexity=40, n_iter=300)\n",
    "tsne50_res = tsne50.fit_transform(pca50_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7.5))\n",
    "ax = sns.scatterplot(x=tsne50_res[:,0], y=tsne50_res[:,1], hue=y['condition'], style=y['condition'], palette=sns.color_palette(\"hls\", nclasses), legend=\"full\", alpha=1, markers=marker_list3, edgecolor='k') # , size=y['condition']\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "ax.set_title('tSNE (using 50 PCA features) - normal transformation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "print([ x for x in range(len(df_samples['label 1'])) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample([4,'andrew',[1,2,3],99,52,'w','d'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_samples['label 1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_1_counts = df_samples['label 1'].value_counts()\n",
    "srs_subset = label_1_counts[[ 'primary tumor' in x.lower() for x in label_1_counts.index ]][:3]\n",
    "print(srs_subset)\n",
    "nsamples_per_condition = [5,3,9]\n",
    "all_indexes_to_use = []\n",
    "for label, nsamples in zip(srs_subset.index, nsamples_per_condition):\n",
    "    condition_indexes = df_samples['label 1'] == label\n",
    "    indexes = np.argwhere(condition_indexes.to_numpy()).flatten()\n",
    "    indexes_to_use = list(indexes[:nsamples])\n",
    "    print('\\nHere are the {} indexes out of {} that correspond to the condition {}:'.format(len(indexes), len(condition_indexes), label))\n",
    "    print(indexes)\n",
    "    print('However, we\\'re only using the first {}:'.format(nsamples))\n",
    "    print(indexes_to_use)\n",
    "    all_indexes_to_use = all_indexes_to_use + indexes_to_use\n",
    "    # curr_df_counts = df_counts.iloc[indexes,:].transpose()\n",
    "    # curr_df_samples = df_samples.iloc[indexes,:].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(all_indexes_to_use)\n",
    "structures_with_indexes = [condition_indexes, df_counts, df_fpkm, df_fpkm_uq, df_samples, df_tpm]\n",
    "for i in range(len(structures_with_indexes)-1):\n",
    "    print(structures_with_indexes[i].index.equals(structures_with_indexes[i+1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_samples_to_use = df_samples.index[all_indexes_to_use]\n",
    "labels_to_use = df_samples.loc[all_samples_to_use, 'label 1']\n",
    "counts_to_use = df_counts.loc[all_samples_to_use,:].transpose()\n",
    "# check the labels are as expected --> done\n",
    "# check the indexes/columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditions_list = []\n",
    "for nsamples, label in zip(nsamples_per_condition, labels_to_use[np.cumsum(nsamples_per_condition) - 1]):\n",
    "    conditions_list = conditions_list + [label]*nsamples\n",
    "conditions_list == labels_to_use.to_list()\n",
    "counts_to_use.columns.equals(labels_to_use.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_directory = '/data/BIDS-HPC/private/projects/dmi2/data'\n",
    "import os\n",
    "with open(file=os.path.join(data_directory, 'annotation.csv'), mode='w') as f:\n",
    "    print('\"file\",\"condition\"', file=f)\n",
    "    for curr_file, condition in zip(labels_to_use.index, labels_to_use):\n",
    "        print('\"{}\",\"{}\"'.format(curr_file, condition), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_to_use = counts_to_use[(counts_to_use!=0).any(axis=1)]\n",
    "with open(file=os.path.join(data_directory, 'gene_counts.tsv'), mode='w') as f:\n",
    "    counts_to_use.iloc[:10,:].to_csv(f, sep='\\t', index_label='gene_id')\n",
    "#counts_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['andrew'] * 3 + ['weisman']*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples.loc['TARGET-52-PAVITI-01A',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_samples.apply(lambda x: x['project id'] + ', ' + x['sample type'], axis='columns')\n",
    "df_samples['label 1'] = df_samples['project id'] + ', ' + df_samples['sample type']\n",
    "df_samples['label 1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_tpm = tc.get_tpm(df_counts, annotation_file)\n",
    "df_tpm1 = tc.get_tpm_from_fpkm(df_fpkm)\n",
    "df_tpm2 = tc.get_tpm_from_fpkm(df_fpkm_uq)\n",
    "import numpy as np\n",
    "print(np.amax(np.abs(df_tpm1-df_tpm).to_numpy(), axis=(0,1)))\n",
    "print(np.amax(np.abs(df_tpm2-df_tpm).to_numpy(), axis=(0,1)))\n",
    "print(np.amax(np.abs(df_tpm2-df_tpm1).to_numpy(), axis=(0,1)))\n",
    "print(np.sqrt(np.mean(((df_tpm1-df_tpm)**2).to_numpy(), axis=(0,1))))\n",
    "print(np.sqrt(np.mean(((df_tpm2-df_tpm)**2).to_numpy(), axis=(0,1))))\n",
    "print(np.sqrt(np.mean(((df_tpm2-df_tpm1)**2).to_numpy(), axis=(0,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_gencode_genes = tc.calculate_exon_lengths(annotation_file)\n",
    "L_srs = df_gencode_genes['exon_length']\n",
    "nsamples, ngenes = df_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_counts.columns.equals(L_srs.index)\n",
    "import numpy as np\n",
    "counts_norm = df_counts / np.tile(np.expand_dims(L_srs, axis=0), (nsamples,1))\n",
    "# test2 = df_counts / L_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = np.tile(np.expand_dims(counts_norm.sum(axis=1), axis=1), (1,ngenes))\n",
    "tpm = counts_norm / denom * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.expand_dims(L_srs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fpkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/'.join(annotation_file.split(sep='/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = df_samples.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in df_samples.index[:100]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Strip the \".gz\" off of the filenames in the \"counts file name\" column of the samples dataframe\n",
    "counts_filenames = [ x.split(sep='.gz')[0] for x in df_samples['counts file name'] ]\n",
    "\n",
    "# For every counts filename in the samples dataframe...\n",
    "for isample, counts_fn in enumerate(counts_filenames):\n",
    "    print(counts_fn)\n",
    "    if isample == 10:\n",
    "        break\n",
    "\n",
    "print(df_samples.index[:10])\n",
    "\n",
    "# The above seems to be different from df_samples.index (seems fine though!), unless I did something weird in make_intensities_dataframes(), below:\n",
    "\n",
    "# import pandas as pd\n",
    "# counts_list = []\n",
    "# for srs in srs_list:\n",
    "#     counts_list.append(pd.DataFrame(srs, index=index))\n",
    "# return(counts_list)\n",
    "\n",
    "#df_samples.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_samples = pd.read_csv('gdc_sample_sheet.2020-07-02.tsv', sep='\\t')\n",
    "sample_is_htseq = df_samples['File Name'].apply(lambda x: 'htseq' in x.lower())\n",
    "sample_has_multiple_cases = df_samples['Case ID'].apply(lambda x: len(x.split())>1)\n",
    "print(df_samples[sample_is_htseq & sample_has_multiple_cases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_samples = pd.read_csv('/data/BIDS-HPC/private/projects/dmi2/data/gdc_sample_sheet.2020-07-02.tsv', sep='\\t')\n",
    "sample_is_htseq = df_samples['File Name'].apply(lambda x: 'htseq' in x.lower())\n",
    "sample_has_multiple_cases = df_samples['Case ID'].apply(lambda x: len(x.split())>1)\n",
    "df_samples[sample_is_htseq & sample_has_multiple_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import relevant library\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "intensity_types = ['counts', 'FPKM', 'FPKM-UQ']\n",
    "nsamples = 5\n",
    "\n",
    "# Get some values from the intensity data\n",
    "nsamples_tot = intensities[0].shape[0]\n",
    "sample_names = intensities[0].index\n",
    "\n",
    "# For each intensity type...\n",
    "for iintensity, intensity_type in enumerate(intensity_types):\n",
    "\n",
    "    # For each of nsamples random samples in the data...\n",
    "    for sample_index in random.sample(range(nsamples_tot), k=nsamples):\n",
    "\n",
    "        # Store the current sample name\n",
    "        sample_name = sample_names[sample_index]\n",
    "\n",
    "        # Get the non-zero intensities for the current sample\n",
    "        srs = intensities[iintensity].iloc[sample_index,:]\n",
    "        srs2 = srs[srs!=0]\n",
    "\n",
    "        # Get a random index of the non-zero intensities and store the corresponding intensity and gene\n",
    "        srs2_index = random.randrange(len(srs2))\n",
    "        intensity = srs2[srs2_index]\n",
    "        gene = srs2.index[srs2_index]\n",
    "\n",
    "        print(sample_index, srs2_index)\n",
    "\n",
    "        # Print what we should see in the files\n",
    "        print('Sample {} should have a {} value of {} for gene {}'.format(sample_name, intensity_type, intensity, gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import relevant library\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "intensity_types = ['counts', 'FPKM', 'FPKM-UQ']\n",
    "nsamples = 5\n",
    "\n",
    "# Get some values from the intensity data\n",
    "nsamples_tot = intensities[0].shape[0]\n",
    "sample_names = intensities[0].index\n",
    "\n",
    "# For each intensity type...\n",
    "for iintensity, intensity_type in enumerate(intensity_types):\n",
    "\n",
    "    # For each of nsamples random samples in the data...\n",
    "    for sample_index in random.sample(range(nsamples_tot), k=nsamples):\n",
    "\n",
    "        # Store the current sample name\n",
    "        sample_name = sample_names[sample_index]\n",
    "\n",
    "        # Get the non-zero intensities for the current sample\n",
    "        srs = intensities[iintensity].iloc[sample_index,:]\n",
    "        srs2 = srs[srs!=0]\n",
    "\n",
    "        # Get a random index of the non-zero intensities and store the corresponding intensity and gene\n",
    "        srs2_index = random.randrange(len(srs2))\n",
    "        intensity = srs2[srs2_index]\n",
    "        gene = srs2.index[srs2_index]\n",
    "\n",
    "        print(sample_index, srs2_index)\n",
    "\n",
    "        # Print what we should see in the files\n",
    "        print('Sample {} should have a {} value of {} for gene {}'.format(sample_name, intensity_type, intensity, gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(counts_fn_holder[697])\n",
    "# print(df_samples.iloc[697,:]['counts file name'])\n",
    "# intensities[0].iloc[697,:]\n",
    "#\n",
    "print(counts_fn_holder[0])\n",
    "#intensities[0]\n",
    "srs_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs_fpkm_uq[128]['ENSG00000213109.4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_samples.iloc[128,:]\n",
    "#df_samples.index.to_list().index('TARGET-10-PASHUP-09A') # TARGET-52-PARRCL-11A\n",
    "print(df_samples.index.to_list().index('TARGET-52-PARRCL-11A'))\n",
    "srs_counts[128]\n",
    "df_samples.iloc[128,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sample_sheet = pd.read_csv(sample_sheet_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_sample_sheet['Sample ID']))[866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for isrs in range(1320):\n",
    "    if not srs_counts[isrs].index.equals(srs_counts[isrs+1].index):\n",
    "        print('uh oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "intensity_types = ['counts', 'FPKM', 'FPKM-UQ']\n",
    "#print(len(df_samples))\n",
    "for iintensity, intensity_type in enumerate(intensity_types):\n",
    "    for sample_index in random.sample(range(len(df_samples)), k=5):\n",
    "        sample = df_samples.index[sample_index]\n",
    "        srs = intensities[iintensity].iloc[sample_index,:]\n",
    "        srs2 = srs[srs!=0]\n",
    "        srs2_index = random.randrange(len(srs2))\n",
    "        intensity = srs2[srs2_index]\n",
    "        gene = srs2.index[srs2_index]\n",
    "        print('Sample {} should have a {} value of {} for gene {}'.format(sample, intensity_type, intensity, gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#c = np.array([[0,10,4], [2,6,12], [33,55,200]])\n",
    "c = np.array([[25,50], [50,0], [25,50]])\n",
    "\n",
    "ngenes, nsamples = c.shape\n",
    "\n",
    "gene_always_expressed = []\n",
    "for igene in range(ngenes):\n",
    "    gene_always_expressed.append(0 not in c[igene,:])\n",
    "\n",
    "c2 = np.log(c[gene_always_expressed,:])\n",
    "\n",
    "ans = c / np.exp(np.median(c2 - c2.mean(axis=1, keepdims=True), axis=0, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans / 35.35533906 * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities[2].index.equals(df_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(indexes_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_per_sample = tc.get_files_per_sample(links_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "srs_counts, srs_fpkm, srs_fpkm_uq = tc.get_intensities(files_per_sample, links_dir, df_gencode_genes, project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples[df_samples['case id'] == 'TARGET-15-SJMPAL042946']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples['case id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ fn for fn in df_samples_to_drop['counts file name'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "indexes_to_drop = []\n",
    "samples_to_drop = []\n",
    "for sample_index in df_samples.index:\n",
    "    sample = df_samples.loc[sample_index]\n",
    "    if len(sample['case id'].split()) > 1: # if there are multiple cases (people; and I've confirmed that they can't be the same people, e.g., living white female and dead black male) corresponding to this one sample...\n",
    "        #print(sample)\n",
    "        indexes_to_drop.append(sample_index)\n",
    "        samples_to_drop.append(sample)\n",
    "df_samples_to_drop = pd.DataFrame(data=samples_to_drop).rename_axis(index='sample id')\n",
    "print(indexes_to_drop)\n",
    "df_samples = df_samples.drop(index=indexes_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'TARGET-30-PASYPX,TARGET-30-PAIXIF'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_names = ['PAIXIF', 'PAPTFZ', 'PANKFE', 'PANUKV']\n",
    "for name in actual_names:\n",
    "    for index in df_samples.index:\n",
    "        if name in index:\n",
    "            print(name, index)\n",
    "# for index in df_samples.index:\n",
    "#     if ' ' in index:\n",
    "#         #print(index)\n",
    "#         print(df_samples.loc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index in df_samples.index:\n",
    "    if ' ' in index:\n",
    "        #print(index)\n",
    "        print(df_samples.loc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_samples.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.full((len(df_samples),), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig_all, ax_all = plt.subplots(nrows=1, ncols=2, figsize=(24,8))\n",
    "ax_hist = df_samples.hist(figsize=(12,8))\n",
    "#ax_hist = df_samples.hist(ax=ax_all[0])\n",
    "nstd = 2\n",
    "# cutoffs = [\n",
    "#     df_samples['average base quality'].mean() - nstd*df_samples['average base quality'].std(),\n",
    "#     df_samples['proportion_base_mismatch'].mean() + nstd*df_samples['proportion_base_mismatch'].std(),\n",
    "#     df_samples['proportion_reads_mapped'].mean() - nstd*df_samples['proportion_reads_mapped'].std()\n",
    "# ]\n",
    "columns = ['average base quality', 'proportion_base_mismatch', 'proportion_reads_mapped']\n",
    "higher_is_better = [True, False, True]\n",
    "sp_locs = [(0,0), (1,1), (2,1)]\n",
    "valid_ind = np.full((len(df_samples),), True)\n",
    "for col, hib, sp_loc in zip(columns, higher_is_better, sp_locs):\n",
    "    sign = -2*int(hib) + 1\n",
    "    vals = df_samples[col]\n",
    "    cutoff = vals.mean() + sign*nstd*vals.std()\n",
    "    ax = ax_hist[sp_loc]\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.plot([cutoff,cutoff], ylim, 'r')\n",
    "\n",
    "    bad_vals = (sign*vals) > (sign*cutoff)\n",
    "    print('There are {} bad values in the \"{}\" plot'.format(sum(bad_vals), col))\n",
    "    valid_ind[bad_vals]=False\n",
    "\n",
    "ntot = len(valid_ind)\n",
    "nbad_tot = ntot - sum(valid_ind)\n",
    "print('Most bad values are overlappling; taken together, there are {} bad values'.format(nbad_tot))\n",
    "print('We should likely use these cutoffs to remove the bad samples; this will only remove {:3.1f}% of the data, leaving {} good samples'.format(nbad_tot/ntot*100, ntot-nbad_tot))\n",
    "print('See for example the two generated images: the first is the original data with the cutoffs plotted in red, and the second is the filtered data with the cutoffs applied')\n",
    "print('For the time being though, we are leaving the data untouched!')\n",
    "\n",
    "ax_hist2 = df_samples.iloc[valid_ind,:].hist(figsize=(12,8))\n",
    "#ax_hist2 = df_samples.iloc[valid_ind,:].hist(ax=ax_all[1])\n",
    "#ax_hist = df_samples.hist(ax=ax_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'andrew': 4, 'weisman': [2,'lee',5]}\n",
    "#'weisman' in mydict.keys()\n",
    "mydict['hey'] = 9\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "df_samples = pd.read_csv('/data/BIDS-HPC/private/projects/dmi2/data/gdc_sample_sheet.2020-07-02.tsv', sep='\\t')\n",
    "with open('/data/BIDS-HPC/private/projects/dmi2/data/metadata.cart.2020-07-02.json') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "filename_mapping_samples = df_samples['File Name']\n",
    "\n",
    "filename_mapping_metadata = []\n",
    "for curr_file in metadata:\n",
    "    filename_mapping_metadata.append(curr_file['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "htseq_suffixes = ['htseq.counts', 'htseq_counts.txt']\n",
    "nfiles_per_sample = []\n",
    "htseq_files_holder = []\n",
    "samples = list(set(df_samples['Sample ID']))\n",
    "index_list = []\n",
    "for sample in samples:\n",
    "    df_sample = df_samples[df_samples['Sample ID']==sample]\n",
    "    nfiles_per_sample.append(len(df_sample))\n",
    "    non_unique_values = (len(df_sample['Data Category'].unique())!=1) or (len(df_sample['Data Type'].unique())!=1) or (len(df_sample['Project ID'].unique())!=1) or (len(df_sample['Case ID'].unique())!=1) or (len(df_sample['Sample ID'].unique())!=1) or (len(df_sample['Sample Type'].unique())!=1)\n",
    "    if non_unique_values:\n",
    "        print('uh oh')\n",
    "        break\n",
    "    htseq_files = [ (fn if ('.'.join(fn.split('.')[1:-1]) in htseq_suffixes) else None) for fn in df_sample['File Name'] ]\n",
    "    #htseq_files_holder.append()\n",
    "\n",
    "    counts_file_list = list(set(htseq_files) - {None})\n",
    "\n",
    "    if len(counts_file_list) == 1:\n",
    "        counts_file = counts_file_list[0]\n",
    "    else:\n",
    "        best_score = -1\n",
    "        best_counts_file = None\n",
    "        for counts_file in counts_file_list:\n",
    "            metadata_index = filename_mapping_metadata.index(counts_file)\n",
    "            score = metadata[metadata_index]['analysis']['input_files'][0]['average_base_quality']\n",
    "            if score > best_score:\n",
    "                best_counts_file = counts_file\n",
    "                best_score = score\n",
    "            #print(counts_file, score)\n",
    "        #print('Using best counts file {}'.format(best_counts_file))\n",
    "        counts_file = best_counts_file\n",
    "\n",
    "    samples_index = filename_mapping_samples[filename_mapping_samples==counts_file].index[0]\n",
    "    metadata_index = filename_mapping_metadata.index(counts_file)\n",
    "    if samples_index != metadata_index:\n",
    "        print('yikes')\n",
    "        break\n",
    "\n",
    "    index_list.append(samples_index)\n",
    "\n",
    "    series = df_samples.loc[samples_index,:]\n",
    "    md1 = metadata[metadata_index]['analysis']['input_files'][0]\n",
    "    ['sample id', 'file list index', 'counts file name', 'average base quality', 'file id', 'project id', 'case id', 'sample type', 'contamination_error', 'proportion_reads_mapped', 'proportion_reads_duplicated', 'contamination', 'proportion_base_mismatch', 'state', 'platform', 'average_read_length', 'entity_submitter_id']\n",
    "    [sample, samples_index, counts_file, best_score, series['File ID'], series['Project ID'], series['Case ID'], series['Sample Type'], md1['contamination_error'], md1['proportion_reads_mapped'], md1['proportion_reads_duplicated'], md1['contamination'], md1['proportion_base_mismatch'], md1['state'], md1['platform'], md1['average_read_length'], metadata[metadata_index]['associated_entities'][0]['entity_submitter_id']]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(samples[-1])\n",
    "print(df_samples.loc[samples_index,'File ID'])\n",
    "print(df_samples.iloc[samples_index,:])\n",
    "metadata[metadata_index]\n",
    "series = df_samples.loc[samples_index,:]\n",
    "md1 = metadata[metadata_index]['analysis']['input_files'][0]\n",
    "['sample id', 'file list index', 'counts file name', 'average base quality', 'file id', 'project id', 'case id', 'sample type', 'contamination_error', 'proportion_reads_mapped', 'proportion_reads_duplicated', 'contamination', 'proportion_base_mismatch', 'state', 'platform', 'average_read_length', 'entity_submitter_id']\n",
    "[sample, samples_index, counts_file, best_score, series['File ID'], series['Project ID'], series['Case ID'], series['Sample Type'], md1['contamination_error'], md1['proportion_reads_mapped'], md1['proportion_reads_duplicated'], md1['contamination'], md1['proportion_base_mismatch'], md1['state'], md1['platform'], md1['average_read_length'], metadata[metadata_index]['associated_entities'][0]['entity_submitter_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[filename_mapping.index('e271fb08-5536-417c-b3e1-45bf8edb05f6.htseq_counts.txt.gz')]['analysis']['input_files'][0]['average_base_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(samples[-1], htseq_files_holder[-1])\n",
    "i = 0\n",
    "for files, sample in zip(htseq_files_holder, samples):\n",
    "    i = i + len(files)\n",
    "    if len(files) > 1:\n",
    "        print(sample, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[0]['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_values = (len(df_sample['Data Category'].unique())!=1) or (len(df_sample['Data Type'].unique())!=1) or (len(df_sample['Project ID'].unique())!=1) or (len(df_sample['Case ID'].unique())!=1) or (len(df_sample['Sample ID'].unique())!=1) or (len(df_sample['Sample Type'].unique())!=1)\n",
    "non_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htseq_suffixes = ['htseq.counts', 'htseq_counts.txt']\n",
    "htseq_files = [ (fn if ('.'.join(fn.split('.')[1:-1]) in htseq_suffixes) else None) for fn in df_sample['File Name'] ]\n",
    "set(htseq_files) - {None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([None, None, 'f42fdca6-1020-4930-84e9-7f1bfbee77a9.htseq_counts.txt.gz', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nfiles_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/data/BIDS-HPC/private/projects/dmi2/data/gdc_sample_sheet.2020-07-02.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "htseq_counts = {'htseq.counts', 'htseq_counts.txt'}\n",
    "star_counts = {'rna_seq.star_gene_counts.tsv'}\n",
    "sample_has_htseq = []\n",
    "sample_has_star = []\n",
    "nhtseq_counts_files = []\n",
    "for isample, sample in enumerate(set(df['Sample ID'])):\n",
    "    all_files_for_sample = [ '.'.join(fn.split('.')[1:-1]) for fn in df[df['Sample ID']==sample]['File Name'] ]\n",
    "    nfiles = len(all_files_for_sample)\n",
    "    all_files_for_sample = set(all_files_for_sample)\n",
    "    sample_has_htseq.append(len(all_files_for_sample-htseq_counts) != nfiles)\n",
    "    sample_has_star.append(len(all_files_for_sample-star_counts) != nfiles)\n",
    "    nhtseq_counts_files.append(nfiles - len(all_files_for_sample-htseq_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(sample_has_htseq)\n",
    "print(set(nhtseq_counts_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "df = pd.read_csv('/home/weismanal/tmp.tsv', sep='\\t', header=None)\n",
    "len_holder = []\n",
    "nextra = 0\n",
    "for sample in set(df[1]):\n",
    "    if len(df[df[1]==sample]) > 1:\n",
    "        print(sample, len(df[df[1]==sample]))\n",
    "        nextra = nextra + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(len_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [2,3,1,6,2,3,5]\n",
    "arr[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(srs_counts)-1):\n",
    "    if srs_counts[i].index.equals(srs_counts[i+1].index):\n",
    "        print('uh oh')\n",
    "#srs_fpkm_uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a{:3.1f}b'.format(24.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "srs_fpkm_uq = list()\n",
    "for counts in srs_counts:\n",
    "    fpkm, fpkm_uq = tc.calculate_fpkm(df_gencode_genes, counts)\n",
    "    srs_fpkm_uq.append(fpkm_uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df_tmp1 = pd.read_csv(os.path.join(links_dir, '0026d9c7-db1e-4cb7-a3d4-4a738a2a96a8.htseq_counts.txt'), sep='\\t', skipfooter=5, names=['id','intensity'])\n",
    "df_tmp2 = pd.read_csv(os.path.join(links_dir, '004fc989-a52b-417b-98c5-f84d7e16a7f5.rna_seq.star_gene_counts.tsv'), sep='\\t', skiprows=5, usecols=[0,1], names=['id','intensity'])\n",
    "df_tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.'.join(['andrew','weisman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#sum([ len(x) for x in files_per_sample ])\n",
    "#counts_suffixes = ['htseq.counts', 'htseq_counts.txt']\n",
    "star_counts_suffix = 'rna_seq.star_gene_counts.tsv'\n",
    "fpkm_suffix = 'FPKM.txt'\n",
    "fpkm_uq_suffix = 'FPKM-UQ.txt'\n",
    "for files in files_per_sample:\n",
    "    counts_file = None\n",
    "    fpkm_file = None\n",
    "    fpkm_uq_file = None\n",
    "    counts_type = None\n",
    "    for ifile, x in enumerate([ curr_file.split('.')[1:] for curr_file in files ]):\n",
    "        suffix = '.'.join(x)\n",
    "        if suffix == fpkm_suffix:\n",
    "            fpkm_file = files[ifile]\n",
    "        elif suffix == fpkm_uq_suffix:\n",
    "            fpkm_uq_file = files[ifile]\n",
    "        else:\n",
    "            if suffix == star_counts_suffix:\n",
    "                counts_type = 'STAR'\n",
    "            else:\n",
    "                counts_type = 'HTSeq'\n",
    "            counts_file = files[ifile]\n",
    "    print('----')\n",
    "    print('Counts file ({}): {}'.format(counts_type, counts_file))\n",
    "    print('FPKM file: {}'.format(fpkm_file))\n",
    "    print('FPKM-UQ file: {}'.format(fpkm_uq_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "# Get a list of all files (with pathnames removed) in links_dir, except for the manifest file\n",
    "txt_files = set([ x.split('/')[-1] for x in glob.glob(os.path.join(links_dir,'*')) ]) - {'MANIFEST.txt'}\n",
    "\n",
    "# Get the corresponding sorted set of basenames (ostensibly, the unique sample names) from the file list\n",
    "basenames = sorted(set([ x.split('.')[0] for x in txt_files ]))\n",
    "\n",
    "\n",
    "files_per_sample2 = []\n",
    "for basename in basenames:\n",
    "    files_per_sample2.append([ x for x in txt_files if basename in x ])\n",
    "\n",
    "files_per_sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['andrew','weisman']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "links_dir = '/data/BIDS-HPC/private/projects/dmi2/data/all_gene_expression_files_in_target/links'\n",
    "txt_files = set([ x.split('/')[-1] for x in glob.glob(os.path.join(links_dir,'*')) ]) - {'MANIFEST.txt'}\n",
    "basenames = sorted(set([ x.split('.')[0] for x in txt_files ]))\n",
    "#basenames[-100:]\n",
    "#glob.glob(os.path.join(links_dir,'f71f8779-c53c-4583-9d5b-b83c157a4f2b')+'*')\n",
    "#print(os.path.join(links_dir,'3163f20-8616-4d2d-b537-bb367418bcfe')+'*')\n",
    "#'c3163f20-8616-4d2d-b537-bb367418bcfe.rna_seq.star_gene_counts.tsv'.split('.')[0]\n",
    "#basenames[:10]\n",
    "#glob.glob(os.path.join(links_dir,'*'))\n",
    "#'/data/BIDS-HPC/private/projects/dmi2/data/all_gene_expression_files_in_target/links/c0d341ba4-980d-40c1-8f57-40ad4dd87e56.htseq.counts'.removeprefix(links_dir)\n",
    "files_per_sample = [ glob.glob(os.path.join(links_dir,basename)+'*') for basename in basenames ]\n",
    "sum([ len(x) for x in files_per_sample ])\n",
    "files_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tc.run_checks(df_gencode_genes, srs_counts[0], fpkm, fpkm_uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(srs_fpkm_uq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(srs_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for df in dfs_counts:\n",
    "    print('hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.to_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_sample_counts[0].index.equals(df_gencode_genes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_counts_arrays[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for sample_counts in XXXX:\n",
    "    # sample_counts = df_samples[0]['intensity']\n",
    "    _, fpkm_uq = tc.calculate_fpkm(df_gencode_genes, sample_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample HT-Seq datafiles\n",
    "datadir = '/data/BIDS-HPC/private/projects/dmi2/data/all_gene_expression_files_in_target/links'\n",
    "file_fpkm = 'fffee315-9aa3-44d2-8c89-78a2c1d107e7.FPKM.txt'\n",
    "file_fpkm_uq = 'fffee315-9aa3-44d2-8c89-78a2c1d107e7.FPKM-UQ.txt'\n",
    "file_counts = 'fffee315-9aa3-44d2-8c89-78a2c1d107e7.htseq_counts.txt'\n",
    "\n",
    "# Define the reference datafiles\n",
    "gdc_tsv_file = '/data/BIDS-HPC/private/projects/dmi2/data/gencode.gene.info.v22.tsv'\n",
    "gencode_gtf_file = '/data/BIDS-HPC/private/projects/dmi2/data/gencode.v22.annotation.gtf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the datafiles into Pandas dataframes\n",
    "import pandas as pd\n",
    "import os\n",
    "df_fpkm = pd.read_csv(os.path.join(datadir, file_fpkm), sep='\\t', names=['id','intensity'])\n",
    "df_fpkm_uq = pd.read_csv(os.path.join(datadir, file_fpkm_uq), sep='\\t', names=['id','intensity'])\n",
    "df_count = pd.read_csv(os.path.join(datadir, file_counts), sep='\\t', skipfooter=5, names=['id','intensity'])\n",
    "df_samples = [df_count, df_fpkm, df_fpkm_uq]\n",
    "df_gdc = pd.read_csv(gdc_tsv_file, sep='\\t')\n",
    "df_gencode = pd.read_csv(gencode_gtf_file, sep='\\t', skiprows=5, header=None)\n",
    "df_gencode_genes = df_gencode[df_gencode[2]=='gene'].reset_index(drop=True)\n",
    "df_gencode_exons = df_gencode[df_gencode[2]=='exon'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the sample dataframes for consistency\n",
    "for idf, df in enumerate(df_samples):\n",
    "    df = df.set_index('id')\n",
    "    df_samples[idf] = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the df_gencode_genes dataframe for consistency\n",
    "df_gencode_genes['id'] = df_gencode_genes.apply(lambda x: x[8].split()[1].split('\\\"')[1], axis=1)\n",
    "df_gencode_genes['type'] = df_gencode_genes.apply(lambda x: x[8].split()[3].split('\\\"')[1], axis=1)\n",
    "df_gencode_genes['name'] = df_gencode_genes.apply(lambda x: x[8].split()[7].split('\\\"')[1], axis=1)\n",
    "df_gencode_genes = df_gencode_genes.rename({3: 'start', 4: 'end', 6: 'strand', 0: 'seqname'}, axis='columns')\n",
    "df_gencode_genes = df_gencode_genes.set_index('id')\n",
    "df_gencode_genes = df_gencode_genes.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the df_gencode_exons dataframe for consistency\n",
    "# Takes about a minute\n",
    "df_gencode_exons['id'] = df_gencode_exons.apply(lambda x: x[8].split()[1].split('\\\"')[1], axis=1)\n",
    "df_gencode_exons['type'] = df_gencode_exons.apply(lambda x: x[8].split()[3].split('\\\"')[1], axis=1)\n",
    "df_gencode_exons['name'] = df_gencode_exons.apply(lambda x: x[8].split()[7].split('\\\"')[1], axis=1)\n",
    "df_gencode_exons = df_gencode_exons.rename({3: 'start', 4: 'end', 6: 'strand', 0: 'seqname'}, axis='columns')\n",
    "df_gencode_exons = df_gencode_exons.set_index('id')\n",
    "df_gencode_exons = df_gencode_exons.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the df_gdc dataframe for consistency\n",
    "df_gdc = df_gdc.rename({'gene_id': 'id', 'gene_name': 'name', 'gene_type': 'type'}, axis='columns')\n",
    "df_gdc = df_gdc.set_index('id')\n",
    "df_gdc = df_gdc.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for column equality between the two reference datafiles\n",
    "for colname in ['name', 'seqname', 'start', 'end', 'strand', 'type']:\n",
    "    print(df_gdc[colname].equals(df_gencode_genes[colname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the ID columns of all five dataframes are exactly the same\n",
    "dfs = df_samples + [df_gdc, df_gencode_genes]\n",
    "ndfs = len(dfs)\n",
    "import numpy as np\n",
    "for idf1 in range(ndfs-1):\n",
    "    for idf2 in np.array(range(ndfs-1-idf1)) + idf1+1:\n",
    "        df1 = dfs[idf1]\n",
    "        df2 = dfs[idf2]\n",
    "        print(idf1, idf2, df1.index.equals(df2.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the exon length of each gene (corresponding to its non-overlapping exons) and add this as a column to the df_gencode_genes dataframe\n",
    "# Takes about 10 minutes\n",
    "\n",
    "# Import relevant library\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of steps to output so we can evaluate progress\n",
    "nsteps = 100\n",
    "\n",
    "# Set the step size in units of the size of the df_gencode_exons dataframe\n",
    "unit_len = int(len(df_gencode_exons) / nsteps)\n",
    "\n",
    "# Initialize some values\n",
    "istep = 0 # the step that we're on\n",
    "exon_lengths = [] # the array holding the final exon gene lengths (non-overlapping union of exon base pairs)\n",
    "prev_idx = '' # set the previous index to null\n",
    "\n",
    "# For every index in the ordered-by-index exons dataframe...\n",
    "for iidx, idx in enumerate(df_gencode_exons.index):\n",
    "\n",
    "    # Get the current row of data in the dataframe\n",
    "    curr_row = df_gencode_exons.iloc[iidx,:]\n",
    "    \n",
    "    # Output progress if the time is right\n",
    "    if (iidx%unit_len) == 0:\n",
    "        print('{}/{} complete...'.format(istep,nsteps))\n",
    "        istep = istep + 1\n",
    "\n",
    "    # If the current index is not equal to the previous index...\n",
    "    if idx != prev_idx:\n",
    "\n",
    "        # If the previous index is not null (i.e., if this isn't the very first loop iteration and therefore base_pairs has been initialized below), calculate and store the number of unique base pairs for the current unique idx\n",
    "        if prev_idx != '':\n",
    "            exon_lengths.append(len(set(np.concatenate(base_pairs))))\n",
    "\n",
    "        # Initialize the base_pairs holder (which will ultimately be a list of lists of base pairs)\n",
    "        base_pairs = []\n",
    "\n",
    "    # Always append the current set of base pairs corresponding to curr_row to the base_pairs list\n",
    "    base_pairs.append(np.arange(curr_row['start'], curr_row['end']+1))\n",
    "\n",
    "    # Set the previous index to the current index\n",
    "    prev_idx = idx\n",
    "\n",
    "# Calculate and store the number of unique base pairs for the final unique idx\n",
    "exon_lengths.append(len(set(np.concatenate(base_pairs))))\n",
    "\n",
    "# Add a column of exon gene length to the genes dataframe\n",
    "df_gencode_genes['exon_length'] = exon_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that we've reproduced what GDC calls the \"exon_length\" and what I'm assuming is probably the \"aggregate_length\" as well\n",
    "#df_gencode_genes = df_gencode_genes.rename({'aggregate_length': 'exon_length'}, axis='columns')\n",
    "print(df_gencode_genes['exon_length'].equals(df_gdc['exon_length']))\n",
    "\n",
    "# Show that using these exon lengths we have achieved adjusted counts that are proportional to the FPKM values\n",
    "tmp = df_samples[0]['intensity'] / df_gencode_genes['exon_length'] / df_samples[1]['intensity']\n",
    "tmp = tmp[tmp.notnull()]\n",
    "print(tmp.std()/tmp.mean()*100, (tmp-tmp.mean()).abs().max()/tmp.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant library\n",
    "import numpy as np\n",
    "\n",
    "# Get the number of reads and gene lengths for the entire set of genes\n",
    "all_counts = df_samples[0]['intensity']\n",
    "all_lengths = df_gencode_genes['exon_length']\n",
    "\n",
    "# Get the number of reads and gene lengths for just the protein-coding genes\n",
    "pc_loc = df_gencode_genes['type'] == 'protein_coding'\n",
    "pc_counts = all_counts[pc_loc]\n",
    "#pc_lengths = all_lengths[pc_loc]\n",
    "\n",
    "# Calculate the normalizations for the FPKM and FPKM-UQ values\n",
    "pc_frag_count = pc_counts.sum()\n",
    "upper_quantile = np.percentile(pc_counts, 75) # equals pc_counts.sort_values()[int(pc_loc.sum()*.75)]\n",
    "\n",
    "# Calculate the normalized counts via https://github.com/NCI-GDC/htseq-tool/blob/master/htseq_tools/tools/fpkm.py\n",
    "tmp = all_counts / all_lengths * 1e9\n",
    "fpkm = tmp / pc_frag_count\n",
    "fpkm_uq = tmp / upper_quantile\n",
    "\n",
    "# Print how well I reproduced the normalized values that I downloaded from the GDC data portal\n",
    "print('Maximum percent error in FPKM: {}'.format((fpkm-df_samples[1]['intensity']).abs().max() / df_samples[1]['intensity'].mean() * 100))\n",
    "print('Maximum percent error in FPKM-UQ: {}'.format((fpkm_uq-df_samples[2]['intensity']).abs().max() / df_samples[2]['intensity'].mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-6\n",
    "((df_fpkm['fpkm']/df_fpkm['fpkm'].sum() - df_fpkm_uq['fpkm-uq']/df_fpkm_uq['fpkm-uq'].sum()).abs() < tol).sum() # This line shows that the FPKM and FPKM-UQ files have the same values up to a fixed normalization\n",
    "((df_counts['count']/df_counts['count'].sum() - df_fpkm_uq['fpkm-uq']/df_fpkm_uq['fpkm-uq'].sum()).abs() < tol).sum() # this line shows that this is not true of the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(df_fpkm['id']) - set(df_gdc['gene_id'])\n",
    "set(df_fpkm['id']) == set(df_gdc['gene_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = df_fpkm['fpkm'].to_numpy()\n",
    "quantiles = np.quantile(arr, [0,0.25,0.5,0.75,1])\n",
    "#arr[quantiles[0]<arr and arr<quantiles[3]]\n",
    "print(arr[(quantiles[3]<arr) & (arr<quantiles[4])].sum())\n",
    "1/(df_fpkm['fpkm'].max()/df_fpkm_uq['fpkm-uq'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.max()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(df_fpkm['fpkm'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gdc.index.equals(df_gencode.index)\n",
    "#df_gdc['name'].equals(df_gencode['name'])\n",
    "#df_gdc['seqname'].equals(df_gencode['seqname'])\n",
    "#df_gdc['start'].equals(df_gencode['start'])\n",
    "#df_gdc['end'].equals(df_gencode['end'])\n",
    "#df_gdc['strand'].equals(df_gencode['strand'])\n",
    "#df_gdc['type'].equals(df_gencode['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({1: [10,11], 2: [20,21]}, index=['a','b'])\n",
    "df2 = pd.DataFrame({1: [11,10], 2: [21,20]}, index=['b','a'])\n",
    "df2.sort_index().equals(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode['id'] = df_gencode.apply(lambda x: x[8].split()[1].split('\\\"')[1], axis=1)\n",
    "df_gencode['type'] = df_gencode.apply(lambda x: x[8].split()[3].split('\\\"')[1], axis=1)\n",
    "df_gencode['name'] = df_gencode.apply(lambda x: x[8].split()[7].split('\\\"')[1], axis=1)\n",
    "df_gencode = df_gencode.rename({3: 'start', 4: 'stop'}, axis='columns')\n",
    "df_gencode = df_gencode.set_index('id')\n",
    "df_gencode = df_gencode.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_samples[0]['intensity'] / df_gdc['exon_length'] / df_samples[1]['intensity']\n",
    "#x = df_samples[2]['intensity'] / df_samples[1]['intensity']\n",
    "x[x.notnull()].std() / x[x.notnull()].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fpkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gencode_full.loc[df_gencode_full[2]=='gene',:].reindex()\n",
    "#df_gencode_full[df_gencode_full[2]=='gene'].equals(pd.read_csv(gencode_gtf_file, sep='\\t', skiprows=5, header=None))\n",
    "df_gencode_full[df_gencode_full[2]=='gene'].reset_index(drop=True).equals(pd.read_csv(gencode_gtf_file, sep='\\t', skiprows=5, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(gencode_gtf_file, sep='\\t', skiprows=5, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id in df_gencode_genes[:5].index:\n",
    "#     print(id)\n",
    "df_gencode_exons.index[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "aggregate_lengths = []\n",
    "prev_idx = ''\n",
    "for iidx, idx in enumerate(df_gencode_exons.index[:100]):\n",
    "    row = df_gencode_exons.iloc[iidx,:]\n",
    "    curr_len = row['end'] - row['start'] + 1\n",
    "    print(row)\n",
    "    if idx != prev_idx:\n",
    "        if prev_idx != '':\n",
    "            aggregate_lengths.append(tmp_len)\n",
    "        tmp_len = 0\n",
    "    tmp_len = tmp_len + curr_len\n",
    "    prev_idx = idx\n",
    "aggregate_lengths.append(tmp_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_exons.index[:100]\n",
    "tmp = df_gencode_exons[df_gencode_exons.index=='ENSG00000000457.12']\n",
    "(tmp['end'] - tmp['start'] + 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [5,5,2,2,2,9,6,6,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_lengths = []\n",
    "prev_num = -1\n",
    "for num in mylist:\n",
    "    if num != prev_num:\n",
    "        if prev_num != -1:\n",
    "            aggregate_lengths.append(tmp_len)\n",
    "        tmp_len = 0\n",
    "    tmp_len = tmp_len + num\n",
    "    prev_num = num\n",
    "aggregate_lengths.append(tmp_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_genes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_exons.index[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_exons[df_gencode_exons.index=='ENSG00000000005.5']\n",
    "#df_gencode_exons[df_gencode_exons.index=='ENSG00000001036.12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "243 - df_gencode_exons[df_gencode_exons.index=='ENSG00000000005.5']['exon_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ivalue, value in enumerate(df_gencode_exons[df_gencode_exons.index=='ENSG00000000005.5'][8]): # ENSGR0000281849.1\n",
    "#for value in df_gencode_exons[df_gencode_exons.index=='ENSGR0000281849.1'][8]:\n",
    "#for value in df_gencode_exons[df_gencode_exons.index=='ENSG00000001036.12'][8]:\n",
    "    print(ivalue, value.split()[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "set(np.concatenate([np.arange(1,5+1), np.arange(4,20+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gencode_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitrenvconda666419ce7e6d428781f855b49005a494",
   "display_name": "Python 3.8.3 64-bit ('r_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}